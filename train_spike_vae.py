import sys
import os

if 'model_spike_vae' in sys.modules:
    del sys.modules['model_spike_vae']

import torch
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from torchvision.utils import save_image
import matplotlib.pyplot as plt
from PIL import Image
import glob
import numpy as np
from datetime import datetime

from model_spike_vae import SpikeVAE, spike_vae_loss, analyze_spike_patterns


class ImageDataset(Dataset):
    """è‡ªå®šä¹‰æ•°æ®é›†ç±»"""
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        
        image_extensions = ['jpg', 'jpeg', 'png', 'bmp', 'gif']
        self.image_paths = []
        
        image_paths_set = set()
        for filename in os.listdir(root_dir):
            file_path = os.path.join(root_dir, filename)
            if os.path.isfile(file_path):
                ext = os.path.splitext(filename)[1].lower().lstrip('.')
                if ext in image_extensions:
                    image_paths_set.add(file_path)
        
        self.image_paths = sorted(list(image_paths_set))
        
        if len(self.image_paths) == 0:
            raise ValueError(f"åœ¨ {root_dir} ä¸­æœªæ‰¾åˆ°ä»»ä½•å›¾ç‰‡æ–‡ä»¶ï¼")
        
        print(f"æ‰¾åˆ° {len(self.image_paths)} å¼ å›¾ç‰‡")
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        try:
            image = Image.open(img_path).convert('RGB')
            if self.transform:
                image = self.transform(image)
            return image, 0
        except Exception as e:
            print(f"åŠ è½½å›¾ç‰‡å¤±è´¥ {img_path}: {e}")
            if self.transform:
                return self.transform(Image.new('RGB', (64, 64), (0, 0, 0))), 0
            return Image.new('RGB', (64, 64), (0, 0, 0)), 0


def main():
    # ========== é…ç½®å‚æ•° ==========
    DATA_DIR = 'D:/lyk/VAE/data/faces'
    IMAGE_SIZE = 128
    BATCH_SIZE = 16
    
    LATENT_DIM = 256
    PRIOR_SPARSITY = 0.1
    
    NUM_EPOCHS = 100
    LEARNING_RATE = 5e-4
    BETA = 1.0
    SPARSITY_WEIGHT = 0.05
    
    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
    CHECKPOINT_DIR = 'checkpoints_spike_vae_fixed'
    VAL_SPLIT = 0.1
    # ==============================
    
    print("=" * 60)
    print("âœ… Spike-VAE æœ€ç»ˆä¿®å¤ç‰ˆè®­ç»ƒç¨‹åº")
    print("=" * 60)
    print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"è®¾å¤‡: {DEVICE}")
    print(f"å›¾åƒå°ºå¯¸: {IMAGE_SIZE}x{IMAGE_SIZE}")
    print(f"éšå˜é‡ç»´åº¦: {LATENT_DIM}")
    print(f"æœŸæœ›ç¨€ç–åº¦: {PRIOR_SPARSITY:.1%}")
    print("=" * 60)
    
    # æ£€æŸ¥æ•°æ®é›†
    if not os.path.exists(DATA_DIR):
        print(f'\né”™è¯¯: æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {DATA_DIR}')
        return
    
    # æ•°æ®é¢„å¤„ç†
    transform = transforms.Compose([
        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),
        transforms.CenterCrop(IMAGE_SIZE),
        transforms.ToTensor(),
    ])
    
    # åŠ è½½æ•°æ®é›†
    print(f'\nåŠ è½½æ•°æ®é›†: {DATA_DIR}')
    full_dataset = ImageDataset(DATA_DIR, transform=transform)
    
    # åˆ†å‰²æ•°æ®é›†
    if VAL_SPLIT > 0:
        val_size = int(len(full_dataset) * VAL_SPLIT)
        train_size = len(full_dataset) - val_size
        train_dataset, val_dataset = torch.utils.data.random_split(
            full_dataset, [train_size, val_size]
        )
        print(f'è®­ç»ƒé›†: {len(train_dataset)}, éªŒè¯é›†: {len(val_dataset)}')
    else:
        train_dataset = full_dataset
        val_dataset = None
        print(f'è®­ç»ƒé›†: {len(train_dataset)}')
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆWindowsä½¿ç”¨num_workers=0ï¼‰
    import platform
    num_workers = 0 if platform.system() == 'Windows' else 2
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=BATCH_SIZE,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=False  # æ”¹ä¸ºFalseï¼Œæ›´å®‰å…¨
    )
    
    val_loader = None
    if val_dataset is not None:
        val_loader = DataLoader(
            val_dataset,
            batch_size=BATCH_SIZE,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=False
        )
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(CHECKPOINT_DIR, exist_ok=True)
    os.makedirs('results_spike_vae_fixed', exist_ok=True)
    
    # åˆ›å»ºæ¨¡å‹
    print('\nåˆ›å»ºæ¨¡å‹...')
    device = torch.device(DEVICE)
    model = SpikeVAE(
        input_channels=3,
        latent_dim=LATENT_DIM,
        prior_sparsity=PRIOR_SPARSITY,
        image_size=IMAGE_SIZE
    ).to(device)
    
    # ä¼˜åŒ–å™¨
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=10, verbose=True
    )
    
    # ä¿å­˜æµ‹è¯•æ ·æœ¬
    test_sample = None
    for data, _ in train_loader:
        test_sample = data[:8].to(device)
        break
    
    print('\nå¼€å§‹è®­ç»ƒ...')
    print('=' * 60)
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(1, NUM_EPOCHS + 1):
        model.train()
        train_loss = 0
        train_recon = 0
        train_kl = 0
        train_actual_sparse = 0
        batch_count = 0
        
        for batch_idx, (data, _) in enumerate(train_loader):
            data = data.to(device)
            
            # ç¡®ä¿æ•°æ®æœ‰æ•ˆ
            data = torch.clamp(data, 0.0, 1.0)
            if torch.isnan(data).any():
                continue
            
            try:
                # å‰å‘ä¼ æ’­
                recon_batch, firing_probs, spikes = model(data, deterministic=False)
                
                # !!! å…³é”®æ£€æŸ¥ï¼šåœ¨æŸå¤±è®¡ç®—å‰å†æ¬¡ç¡®è®¤firing_probsèŒƒå›´
                if (firing_probs < 0).any() or (firing_probs > 1).any():
                    print(f'\nâš ï¸  æ£€æµ‹åˆ°firing_probsè¶…å‡ºèŒƒå›´!')
                    print(f'   èŒƒå›´: [{firing_probs.min().item()}, {firing_probs.max().item()}]')
                    print(f'   å¼ºåˆ¶è£å‰ªå¹¶ç»§ç»­...')
                    firing_probs = torch.clamp(firing_probs, 0.0, 1.0)
                
                # è®¡ç®—æŸå¤±
                loss, recon_loss, kl_loss, sparsity_loss, actual_sparsity = spike_vae_loss(
                    recon_batch, data, firing_probs, spikes,
                    prior_sparsity=model.prior_sparsity,
                    beta=BETA,
                    sparsity_weight=SPARSITY_WEIGHT
                )
                
                # æ£€æŸ¥æŸå¤±æœ‰æ•ˆæ€§
                if torch.isnan(loss) or torch.isinf(loss):
                    print(f'\nâš ï¸  æŸå¤±æ— æ•ˆï¼Œè·³è¿‡batch {batch_idx}')
                    continue
                
                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                optimizer.step()
                
                # ç´¯è®¡
                train_loss += loss.item()
                train_recon += recon_loss.item()
                train_kl += kl_loss.item()
                train_actual_sparse += actual_sparsity.item()
                batch_count += 1
                
                # æ—¥å¿—
                if batch_idx % 50 == 0:
                    print(f'Epoch {epoch}/{NUM_EPOCHS} [{batch_idx * len(data)}/{len(train_loader.dataset)} '
                          f'({100. * batch_idx / len(train_loader):.0f}%)] '
                          f'Loss: {loss.item():.4f} | '
                          f'Recon: {recon_loss.item():.4f} | '
                          f'KL: {kl_loss.item():.4f} | '
                          f'Sparsity: {actual_sparsity.item():.2%}')
            
            except RuntimeError as e:
                if 'CUDA' in str(e) or 'assert' in str(e).lower():
                    print(f'\nâŒ CUDAé”™è¯¯åœ¨epoch {epoch}, batch {batch_idx}')
                    print(f'   é”™è¯¯: {e}')
                    print(f'   è·³è¿‡æ­¤batchå¹¶ç»§ç»­...')
                    continue
                else:
                    raise e
        
        # è®¡ç®—å¹³å‡
        if batch_count > 0:
            avg_train_loss = train_loss / batch_count
            avg_train_recon = train_recon / batch_count
            avg_train_kl = train_kl / batch_count
            avg_train_actual_sparse = train_actual_sparse / batch_count
        else:
            print(f'\nâš ï¸  Epoch {epoch}: æ‰€æœ‰batchéƒ½å¤±è´¥äº†ï¼')
            continue
        
        # éªŒè¯é˜¶æ®µ
        if val_loader is not None:
            model.eval()
            val_loss = 0
            val_actual_sparse = 0
            val_count = 0
            
            with torch.no_grad():
                for data, _ in val_loader:
                    data = data.to(device)
                    data = torch.clamp(data, 0.0, 1.0)
                    
                    try:
                        recon_batch, firing_probs, spikes = model(data, deterministic=True)
                        loss, _, _, _, actual_sparsity = spike_vae_loss(
                            recon_batch, data, firing_probs, spikes,
                            prior_sparsity=model.prior_sparsity,
                            beta=BETA,
                            sparsity_weight=SPARSITY_WEIGHT
                        )
                        
                        val_loss += loss.item()
                        val_actual_sparse += actual_sparsity.item()
                        val_count += 1
                    except:
                        continue
            
            if val_count > 0:
                avg_val_loss = val_loss / val_count
                avg_val_sparse = val_actual_sparse / val_count
                scheduler.step(avg_val_loss)
                
                print(f'\n{"="*60}')
                print(f'Epoch {epoch} Summary:')
                print(f'  Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}')
                print(f'  Train Sparsity: {avg_train_actual_sparse:.2%} | Val Sparsity: {avg_val_sparse:.2%}')
                print(f'  Target Sparsity: {PRIOR_SPARSITY:.2%}')
                print(f'{"="*60}\n')
        else:
            print(f'\n{"="*60}')
            print(f'Epoch {epoch}: Loss: {avg_train_loss:.4f}, Sparsity: {avg_train_actual_sparse:.2%}')
            print(f'{"="*60}\n')
        
        # ä¿å­˜é‡æ„æ ·æœ¬
        if epoch % 5 == 0 or epoch == 1:
            model.eval()
            with torch.no_grad():
                try:
                    recon_sample, firing_probs_sample, spikes_sample = model(test_sample, deterministic=True)
                    
                    # ç®€å•å¯¹æ¯”å›¾
                    comparison = torch.cat([test_sample, recon_sample], dim=0)
                    save_image(comparison, 
                              f'results_spike_vae_fixed/reconstruction_epoch_{epoch}.png', 
                              nrow=8)
                    
                    recon_error = F.mse_loss(recon_sample, test_sample).item()
                    actual_sparsity_sample = spikes_sample.mean().item()
                    
                    print(f'\nâœ“ å·²ä¿å­˜é‡æ„æ ·æœ¬ (Epoch {epoch}):')
                    print(f'  - reconstruction_epoch_{epoch}.png')
                    print(f'  - é‡æ„è¯¯å·® (MSE): {recon_error:.6f}')
                    print(f'  - ç¨€ç–åº¦: {actual_sparsity_sample:.2%}')
                except Exception as e:
                    print(f'\nâš ï¸  ä¿å­˜é‡æ„æ ·æœ¬å¤±è´¥: {e}')
        
        # ä¿å­˜æ¨¡å‹
        if epoch % 10 == 0 or epoch == NUM_EPOCHS:
            checkpoint_path = os.path.join(CHECKPOINT_DIR, f'spike_vae_epoch_{epoch}.pth')
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': avg_train_loss,
                'latent_dim': LATENT_DIM,
                'prior_sparsity': PRIOR_SPARSITY,
                'beta': BETA,
                'sparsity_weight': SPARSITY_WEIGHT,
                'image_size': IMAGE_SIZE,
            }, checkpoint_path)
            print(f'âœ“ æ¨¡å‹å·²ä¿å­˜: {checkpoint_path}')
    
    print("\n" + "=" * 60)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print("=" * 60)
    print(f"æ¨¡å‹ä¿å­˜åœ¨: {CHECKPOINT_DIR}/")
    print(f"é‡æ„æ ·æœ¬åœ¨: results_spike_vae_fixed/")


if __name__ == '__main__':
    print("\nğŸ”„ å¼ºåˆ¶é‡æ–°åŠ è½½æ¨¡å—...")
    print("   è¿™ç¡®ä¿ä½¿ç”¨æœ€æ–°çš„ä¿®å¤ä»£ç \n")
    main()

